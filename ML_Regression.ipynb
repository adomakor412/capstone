{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/adomakor412/conda/envs/MyEnv/lib/python3.6/site-packages/dask/config.py:131: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import path as op\n",
    "import itertools\n",
    "import re\n",
    "import xarray as xr\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy.random as nr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = '/scratch/adomakor412/all_npy3'\n",
    "PATH = os.path.expanduser(inputPath)\n",
    "ncPath = os.path.expanduser('/scratch/adomakor412/april_data_2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"time\",\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    #\"band\",#Necessary?\n",
    "    \"G17_Temp\",\n",
    "    \"G17_mean\",\n",
    "    \"G17_std\",#mean and std outside inner for loop for comp. eff.\n",
    "    \"target_G16_Temp\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({},columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longitude(lonMin, lonMax, col, colSize):\n",
    "    lon = (col/colSize)* (lonMax - lonMin)\n",
    "    return lon\n",
    "\n",
    "def latitude(latMin, latMax, row, rowSize):\n",
    "    lat = (row/rowSize)* (latMax - latMin)\n",
    "    return lat\n",
    "\n",
    "def Rad2BT(rad, planck_fk1, planck_fk2, planck_bc1, planck_bc2):\n",
    "    \"\"\"Radiances to Brightness Temprature (using black body equation)\"\"\"\n",
    "    invRad = np.array(rad)**(-1)\n",
    "    arg = (invRad*planck_fk1) + 1.0\n",
    "    T = (- planck_bc1+(planck_fk2 * (np.log(arg)**(-1))) )*(1/planck_bc2) \n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extent_pc = [-109.59326, -102.40674, 8.94659, -8.94656]\n",
    "\n",
    "dataFrames = []\n",
    "randPool = 401401\n",
    "randSize = 1000#randPool*.1\n",
    "inds=nr.randint(randPool, size=(randSize))\n",
    "inds.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR_ABI-L1b-RadF-M6C08_G17_s20191030000339_e20191030009405_c20191030009441.nc\n"
     ]
    }
   ],
   "source": [
    "MAE = []\n",
    "MSE = []\n",
    "R2 = []\n",
    "R2_train_test = []\n",
    "\n",
    "dfFixedTest = []\n",
    "\n",
    "trainSamp = [100, 1000, 10000, 100000]\n",
    "for samp in trainSamp:\n",
    "    with open('logML', 'a') as log:\n",
    "        dataFrames = []\n",
    "        for (bb,dd) in list(itertools.product([8],[5])):#Let's start with one day\n",
    "            DD = str(98+dd).zfill(3)\n",
    "            lookup = f'M6C08_G17_s2019{DD}0000'\n",
    "            ncFiles = [f for f in os.listdir(ncPath) if re.search(lookup,f)]\n",
    "            #ncFiles = mySort(ncFiles)\n",
    "            npFiles = [f for f in os.listdir( PATH ) if re.search(lookup,f)]\n",
    "            #ncFiles = mySort(ncFiles)\n",
    "\n",
    "            for ncf, npf in zip(ncFiles,npFiles):#can refactor to just conversion of selected files\n",
    "                try:\n",
    "                    imageBox = np.load(op.join( PATH,npf))\n",
    "                    myFile = xr.open_dataset(op.join(ncPath,ncf))\n",
    "                    planck_fk1 = float(myFile['planck_fk1'].data)\n",
    "                    planck_fk2 = float(myFile['planck_fk2'].data) \n",
    "                    planck_bc1 = float(myFile['planck_bc1'].data)                       \n",
    "                    planck_bc2 = float(myFile['planck_bc2'].data)\n",
    "\n",
    "                    time = ncf[31:38]\n",
    "                    G17_mean = Rad2BT(imageBox.mean(), planck_fk1, planck_fk2, planck_bc1, planck_bc2)\n",
    "                    G17_std = Rad2BT(imageBox.std(), planck_fk1, planck_fk2, planck_bc1, planck_bc2)\n",
    "\n",
    "                    hh = ncf[34:36]\n",
    "                    mm = ncf[36:38]\n",
    "\n",
    "                    print(ncf)\n",
    "                    print(str(ncf), file=log)\n",
    "                    logger.info(str(ncf))\n",
    "\n",
    "                    G16_npy = np.load( op.join(PATH, npf.replace('G16','G17',1)) )\n",
    "                    G16_ncf = xr.open_dataset(op.join( ncPath, ncf.replace('G16','G17',1) ))\n",
    "                    G16_fk1 = float(G16_ncf['planck_fk1'].data)\n",
    "                    G16_fk2 = float(G16_ncf['planck_fk2'].data) \n",
    "                    G16_bc1 = float(G16_ncf['planck_bc1'].data)                       \n",
    "                    G16_bc2 = float(G16_ncf['planck_bc2'].data)\n",
    "\n",
    "                    target_G16_Temp = Rad2BT(G16_npy, G16_fk1, G16_fk2, G16_bc1, G16_bc2)\n",
    "                    x,y = imageBox.shape[0],imageBox.shape[1]\n",
    "                    randInds = nr.randint(10000, size=(x*y))\n",
    "\n",
    "                    #select \"10,000\" random points out of 401,401\n",
    "                    sample = np.array([combo for combo in itertools.product(range(x),range(y))])\n",
    "                    for i,j in sample:\n",
    "                        lon = longitude( extent_pc[0], extent_pc[1], i, x )\n",
    "                        lat = latitude( extent_pc[2], extent_pc[3], j, y)\n",
    "\n",
    "                        G17_Temp = Rad2BT(imageBox[i,j], planck_fk1, planck_fk2, planck_bc1, planck_bc2)#unfiltered\n",
    "                        target_G16_Temp = Rad2BT( G16_npy[i,j], G16_fk1, G16_fk2, G16_bc1, G16_bc2 )\n",
    "\n",
    "                        row = [time, lon, lat, G17_Temp, G17_mean, G17_std, target_G16_Temp]\n",
    "\n",
    "                        dataFrames.append(pd.DataFrame([row],columns=columns))\n",
    "                        #concatenate a list of single dataframes after done with loop for speed \" pandas.concat\"\n",
    "                        #z score versus the standard deviation for single image, can normalize across all image\n",
    "\n",
    "                except ValueError as e:\n",
    "                    logger.exception(e)\n",
    "                    print(e)\n",
    "                    print(e, file=log)\n",
    "                    continue\n",
    "                 \n",
    "        \n",
    "        if len(dfFixedTest)==0:\n",
    "            dfFixedTest = df.sample(1000)\n",
    "            X_test = dfFixedTest.drop([\"target_G16_Temp\"],axis=1).astype('float')\n",
    "            y_test = np.ravel(dfFixedTest.drop(columns[:-1],axis=1).astype('float'))\n",
    "\n",
    "        train = regr.score(X_train,y_train)\n",
    "        test = regr.score(X_test, y_test)\n",
    "\n",
    "        R2.append(train)\n",
    "        R2_train_test.append(train/test)\n",
    "\n",
    "        prediction = regr.predict(X_train)\n",
    "        errors = abs(prediction - y_train)\n",
    "\n",
    "        MAE.append( round(np.mean(errors), 2))\n",
    "\n",
    "        MSE.append(mean_squared_error(prediction, y_test))\n",
    "        \n",
    "        print('MAE', MAE)\n",
    "        print('MSE', MSE)\n",
    "        print('R2',R2)\n",
    "        print('R2 train to test ratio',R2_train_test)\n",
    "# df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", names=columns)\n",
    "# for column in df.columns:\n",
    "#     if df[column].dtype == \"object\":\n",
    "#         df[column] = df[column].str.strip()\n",
    "# df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE', MAE)\n",
    "print('MSE', MSE)\n",
    "print('R2',R2)\n",
    "print('R2 train to test ratio',R2_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = '/scratch/adomakor412/july_26-27_2019'\n",
    "PATH = os.path.expanduser(inputPath)\n",
    "ncPath = os.path.expanduser('/scratch/adomakor412/july_26-27_2019')\n",
    "\n",
    "MAE_ML = []\n",
    "MSE_ML = []\n",
    "R2_ML = []\n",
    "R2_ML_train_test = []\n",
    "\n",
    "MAE_pCal = []\n",
    "MSE_pCal = []\n",
    "R2_pCal = []\n",
    "R2_pCal_train_test = []\n",
    "\n",
    "dfFixedTest_ML = []#df.iloc[[inds]]\n",
    "dfFixedTest_pCal = []#\n",
    "\n",
    "trainSamp = [100, 1000, 10000, 100000]\n",
    "for samp in trainSamp:\n",
    "    dataFrames = []\n",
    "    with open('logML', 'a') as log:\n",
    "\n",
    "        for (bb,dd,SS) in list(itertools.product([8],[207,208],[17])):\n",
    "            DD = str(dd).zfill(3)\n",
    "            lookup = f'M6C08_G{SS}_s2019{DD}'\n",
    "            ncFiles = [f for f in os.listdir(ncPath) if re.search(lookup,f)]\n",
    "            #ncFiles = mySort(ncFiles)\n",
    "            npFiles = [f for f in os.listdir( PATH ) if re.search(lookup,f)]\n",
    "            #ncFiles = mySort(ncFiles)\n",
    "            \n",
    "            randInds = nr.randint(samp, size=(x*y))\n",
    "            randInds.sort()\n",
    "\n",
    "            for ncf, npf in zip(ncFiles,npFiles):#for ncf, npf in zip(ncFiles,npFiles)[randInds] day sample <=576\n",
    "                try:\n",
    "                    imageBox = np.load(op.join( PATH,npf))\n",
    "                    myFile = xr.open_dataset(op.join(ncPath,ncf))\n",
    "                    planck_fk1 = float(myFile['planck_fk1'].data)\n",
    "                    planck_fk2 = float(myFile['planck_fk2'].data) \n",
    "                    planck_bc1 = float(myFile['planck_bc1'].data)                       \n",
    "                    planck_bc2 = float(myFile['planck_bc2'].data)\n",
    "\n",
    "                    time = ncf[31:38]\n",
    "                    G17_mean = Rad2BT(imageBox.mean(), planck_fk1, planck_fk2, planck_bc1, planck_bc2)\n",
    "                    G17_std = Rad2BT(imageBox.std(), planck_fk1, planck_fk2, planck_bc1, planck_bc2)\n",
    "\n",
    "                    hh = ncf[34:36]\n",
    "                    mm = ncf[36:38]\n",
    "\n",
    "                    print(ncf)\n",
    "                    print(str(ncf), file=log)\n",
    "                    logger.info(str(ncf))\n",
    "\n",
    "                    G16_npy = np.load( op.join(PATH, npf.replace('G16','G17',1)) )\n",
    "                    G16_ncf = xr.open_dataset(op.join( ncPath, ncf.replace('G16','G17',1) ))\n",
    "                    G16_fk1 = float(G16_ncf['planck_fk1'].data)\n",
    "                    G16_fk2 = float(G16_ncf['planck_fk2'].data) \n",
    "                    G16_bc1 = float(G16_ncf['planck_bc1'].data)                       \n",
    "                    G16_bc2 = float(G16_ncf['planck_bc2'].data)\n",
    "\n",
    "                    target_G16_Temp = Rad2BT(G16_npy, G16_fk1, G16_fk2, G16_bc1, G16_bc2)\n",
    "                    x,y = imageBox.shape[0],imageBox.shape[1]\n",
    "\n",
    "                    #select \"10,000\" random points out of 401,401\n",
    "                    sample = np.array([combo for combo in itertools.product(range(x),range(y))])\n",
    "                    for i,j in sample:\n",
    "                        lon = longitude( extent_pc[0], extent_pc[1], i, x )\n",
    "                        lat = latitude( extent_pc[2], extent_pc[3], j, y)\n",
    "\n",
    "                        G17_Temp = Rad2BT(imageBox[i,j], planck_fk1, planck_fk2, planck_bc1, planck_bc2)#unfiltered\n",
    "                        target_G16_Temp = Rad2BT( G16_npy[i,j], G16_fk1, G16_fk2, G16_bc1, G16_bc2 )\n",
    "\n",
    "                        row = [time, lon, lat, G17_Temp, G17_mean, G17_std, target_G16_Temp]\n",
    "\n",
    "                        dataFrames.append(pd.DataFrame([row],columns=columns))\n",
    "                        #concatenate a list of single dataframes after done with loop for speed \" pandas.concat\"\n",
    "                        #z score versus the standard deviation for single image, can normalize across all image\n",
    "\n",
    "                except ValueError as e:\n",
    "                    logger.exception(e)\n",
    "                    print(e)\n",
    "                    print(e, file=log)\n",
    "                    continue\n",
    "        df=pd.concat(dataFrames)\n",
    "        df = df.dropna()\n",
    "        \n",
    "        from sklearn.neural_network import MLPRegressor\n",
    "        #from sklearn.datasets import make_regression\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        \n",
    "        dfSample = df.sample(n=samp)\n",
    "        X = dfSample.drop([\"target_G16_Temp\"],axis=1).astype('float')#try as float\n",
    "        y = dfSample.drop(columns[:-1],axis=1).astype('float')\n",
    "            \n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, np.ravel(y),random_state=1,test_size=0.0)\n",
    "        regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        \n",
    "        If dd == 207:#non pCal\n",
    "            if len(dfFixedTest_ML)==0:\n",
    "                dfFixedTest_ML = df.sample(1000)\n",
    "                X_test = dfFixedTest_ML.drop([\"target_G16_Temp\"],axis=1).astype('float')\n",
    "                y_test = np.ravel(dfFixedTest_ML.drop(columns[:-1],axis=1).astype('float'))\n",
    "            \n",
    "            train = regr.score(X_train,y_train)\n",
    "            test = regr.score(X_test, y_test)\n",
    "            \n",
    "            R2_ML.append(train)\n",
    "            R2_ML_train_test.append(train/test)\n",
    "            \n",
    "            prediction = regr.predict(X_train)\n",
    "            errors = abs(prediction - y_train)\n",
    "            \n",
    "            MAE = round(np.mean(errors), 2)\n",
    "            MAE_ML.append(MAE)\n",
    "            \n",
    "            MSE = mean_squared_error(prediction, y_test)\n",
    "            MSE_ML.append(MSE)\n",
    "            \n",
    "        If dd == 208:#pCal        \n",
    "            if len(dfFixedTest_pCal)==0:\n",
    "                dfFixedTest_pCal = df.sample(1000)\n",
    "                X_test = dfFixedTest_pCal.drop([\"target_G16_Temp\"],axis=1).astype('float')\n",
    "                y_test = np.ravel(dfFixedTest_pCal.drop(columns[:-1],axis=1).astype('float'))\n",
    "            \n",
    "            train = regr.score(X_train,y_train)\n",
    "            test = regr.score(X_test, y_test)\n",
    "            \n",
    "            R2_pCal.append(train)\n",
    "            R2_pCal_train_test.append(train/test)\n",
    "            \n",
    "            prediction = regr.predict(X_train)\n",
    "            errors = abs(prediction - y_train)\n",
    "            \n",
    "            MAE = round(np.mean(errors), 2)\n",
    "            MAE_pCal.append(MAE)\n",
    "            \n",
    "            MSE = mean_squared_error(prediction, y_test)\n",
    "            MSE_pCal.append(MSE)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat(dataFrames)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "#from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample = df.sample(n=10000)\n",
    "X = dfSample.drop([\"target_G16_Temp\"],axis=1).astype('float')#try as float\n",
    "y = dfSample.drop(columns[:-1],axis=1).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, np.ravel(y),random_state=1,test_size=0.0)\n",
    "regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = dfFixedTest.drop([\"target_G16_Temp\"],axis=1).astype('float')\n",
    "y_test = np.ravel(dfFixedTest.drop(columns[:-1],axis=1).astype('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = abs(prediction - y_test)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'Kelvin')\n",
    "print('Mean Square Error:', mean_squared_error(prediction, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous MSE was 23.96 at 1000 points sampled. Going from 1000 to 10000 points acheived better MSE (23.96 -> 0.36), MAE (3.02 Kelvins -> 0.52 Kelvins) and R^2 of both the test (-0.42 -> 0.61) and train (-0.32 ->0.6) data.\n",
    "\n",
    "### Since the R^2 of the train and test data are similar, we know we are not overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(itertools.product([8],[207,208],[16,17]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
