{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import path as op\n",
    "import itertools\n",
    "import re\n",
    "import xarray as xr\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy.random as nr\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = '/scratch/adomakor412/july_25-26_2019_npy'\n",
    "PATH = os.path.expanduser(inputPath)\n",
    "ncPath = os.path.expanduser('/scratch/adomakor412/july_25-26_2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"time\",\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    #\"band\",#Necessary?\n",
    "    \"G17_Temp\",\n",
    "    \"G17_mean\",\n",
    "    \"G17_std\",#mean and std outside inner for loop for comp. eff.\n",
    "    \"target_G16_Temp\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({},columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longitude(lonMin, lonMax, col, colSize):\n",
    "    lon = (col/colSize)* (lonMax - lonMin)\n",
    "    return lon\n",
    "\n",
    "def latitude(latMin, latMax, row, rowSize):\n",
    "    lat = (row/rowSize)* (latMax - latMin)\n",
    "    return lat\n",
    "\n",
    "def Rad2BT(rad, planck_fk1, planck_fk2, planck_bc1, planck_bc2):\n",
    "    \"\"\"Radiances to Brightness Temprature (using black body equation)\"\"\"\n",
    "    invRad = np.array(rad)**(-1)\n",
    "    arg = (invRad*planck_fk1) + 1.0\n",
    "    T = (- planck_bc1+(planck_fk2 * (np.log(arg)**(-1))) )*(1/planck_bc2) \n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extent_pc = [-109.59326, -102.40674, 8.94659, -8.94656]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a fixed test set of size 10^5 of random pixels to be compared to training sets. We create a fixed test set for both MLP and pCal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFixed_ML = []#df.iloc[[inds]]\n",
    "dfFixed_pCal = []\n",
    "\n",
    "trainSamp = [100000]\n",
    "\n",
    "for samp in trainSamp:\n",
    "    dataFrames = []\n",
    "    with open('logML', 'a') as log:\n",
    "        for (bb,dd,SS) in list(itertools.product([8],[206,207],[17])):\n",
    "            DD = str(dd).zfill(3)\n",
    "            lookup = f'M6C08_G{SS}_s2019{DD}1200'\n",
    "            ncFiles = [f for f in os.listdir(ncPath) if re.search(lookup,f)]\n",
    "            #ncFiles = mySort(ncFiles)\n",
    "            npFiles = [f for f in os.listdir( PATH ) if re.search(lookup,f)] #Numpy files not fully data cleaned\n",
    "            #ncFiles = mySort(ncFiles)\n",
    "            \n",
    "            #randInds = nr.randint(samp, size=(x*y))\n",
    "            #randInds.sort()\n",
    "\n",
    "            for ncf, npf in zip(ncFiles,npFiles):#for ncf, npf in zip(ncFiles,npFiles)[randInds] day sample <=576\n",
    "                try:\n",
    "                    imageBox = np.load(op.join( PATH,npf))\n",
    "                    myFile = xr.open_dataset(op.join(ncPath,ncf))\n",
    "                    planck_fk1 = float(myFile['planck_fk1'].data)\n",
    "                    planck_fk2 = float(myFile['planck_fk2'].data) \n",
    "                    planck_bc1 = float(myFile['planck_bc1'].data)                       \n",
    "                    planck_bc2 = float(myFile['planck_bc2'].data)\n",
    "\n",
    "                    time = ncf[31:38]\n",
    "                    G17_mean = Rad2BT(imageBox.mean(), planck_fk1, planck_fk2, planck_bc1, planck_bc2)\n",
    "                    G17_std = Rad2BT(imageBox.std(), planck_fk1, planck_fk2, planck_bc1, planck_bc2)\n",
    "\n",
    "                    hh = ncf[34:36]\n",
    "                    mm = ncf[36:38]\n",
    "\n",
    "                    print(str(ncf), file=log)\n",
    "                    logger.info(str(ncf))\n",
    "\n",
    "                    G16_npy = np.load( op.join(PATH, npf.replace('G16','G17',1)) )\n",
    "                    G16_ncf = xr.open_dataset(op.join( ncPath, ncf.replace('G16','G17',1) ))\n",
    "                    G16_fk1 = float(G16_ncf['planck_fk1'].data)\n",
    "                    G16_fk2 = float(G16_ncf['planck_fk2'].data) \n",
    "                    G16_bc1 = float(G16_ncf['planck_bc1'].data)                       \n",
    "                    G16_bc2 = float(G16_ncf['planck_bc2'].data)\n",
    "\n",
    "                    target_G16_Temp = Rad2BT(G16_npy, G16_fk1, G16_fk2, G16_bc1, G16_bc2)\n",
    "                    x,y = imageBox.shape[0],imageBox.shape[1]\n",
    "\n",
    "                    #select \"10,000\" random points out of 401,401\n",
    "                    sample = np.array([combo for combo in itertools.product(range(x),range(y))])\n",
    "                    inds=nr.randint(x*y, size=(samp))\n",
    "                    inds.sort()\n",
    "                    \n",
    "                    for i,j in sample[inds]:#bottleneck for time\n",
    "                        lon = longitude( extent_pc[0], extent_pc[1], i, x )\n",
    "                        lat = latitude( extent_pc[2], extent_pc[3], j, y)\n",
    "\n",
    "                        G17_Temp = Rad2BT(imageBox[i,j], planck_fk1, planck_fk2, planck_bc1, planck_bc2)#nonfilter\n",
    "                        target_G16_Temp = Rad2BT( G16_npy[i,j], G16_fk1, G16_fk2, G16_bc1, G16_bc2 )\n",
    "\n",
    "                        row = [time, lon, lat, G17_Temp, G17_mean, G17_std, target_G16_Temp]\n",
    "\n",
    "                        dataFrames.append(pd.DataFrame([row],columns=columns))\n",
    "                        #concatenate a list of single dataframes after done with loop for speed \" pandas.concat\"\n",
    "                        #z score versus the standard deviation for single image, can normalize across all image\n",
    "                 \n",
    "                    print('Wrapping up on sample')\n",
    "\n",
    "                    df=pd.concat(dataFrames)\n",
    "                    df = df.dropna()\n",
    "                    df = df[ df[\"G17_Temp\"] > 0]\n",
    "                    df = df[ df[\"target_G16_Temp\"] > 0]\n",
    "\n",
    "                    dfSample = df#df.sample(n=samp)\n",
    "\n",
    "                    if int(DD) == 206:#non pCal\n",
    "                        print('Working on MLP sample')\n",
    "                        dfFixed_ML = dfSample\n",
    "\n",
    "                    if int(DD) == 207:#pCal        \n",
    "                        print('Working on pCal sample')\n",
    "                        dfFixed_pCal = dfSample\n",
    "                        \n",
    "                except ValueError as e:\n",
    "                    logger.exception(e)\n",
    "                    print(e)\n",
    "                    print(e, file=log)\n",
    "                    continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we generate the training sets of various sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "inputPath = '/scratch/adomakor412/july_25-26_2019_npy'\n",
    "PATH = os.path.expanduser(inputPath)\n",
    "ncPath = os.path.expanduser('/scratch/adomakor412/july_25-26_2019')\n",
    "\n",
    "MAE_ML = []\n",
    "MSE_ML = []\n",
    "R2_ML = []\n",
    "R2_ML_train_test = []\n",
    "\n",
    "MAE_pCal = []\n",
    "MSE_pCal = []\n",
    "R2_pCal = []\n",
    "R2_pCal_train_test = []\n",
    "\n",
    "dfTarget_ML = []#df.iloc[[inds]]\n",
    "dfTarget_pCal = []\n",
    "dfPredict_ML = []\n",
    "dfPredict_pCal = []\n",
    "\n",
    "trainSamp = [100, 1000, 10000, 100000]\n",
    "\n",
    "for samp in trainSamp:\n",
    "    dataFrames = []\n",
    "    with open('logML', 'a') as log:\n",
    "        for (bb,dd,SS) in list(itertools.product([8],[206,207],[17])):\n",
    "            DD = str(dd).zfill(3)\n",
    "            lookup = f'M6C08_G{SS}_s2019{DD}1200'\n",
    "            ncFiles = [f for f in os.listdir(ncPath) if re.search(lookup,f)]\n",
    "            #ncFiles = mySort(ncFiles)\n",
    "            npFiles = [f for f in os.listdir( PATH ) if re.search(lookup,f)]\n",
    "            #ncFiles = mySort(ncFiles)\n",
    "            \n",
    "            #randInds = nr.randint(samp, size=(x*y))\n",
    "            #randInds.sort()\n",
    "\n",
    "            for ncf, npf in zip(ncFiles,npFiles):#for ncf, npf in zip(ncFiles,npFiles)[randInds] day sample <=576\n",
    "                try:\n",
    "                    imageBox = np.load(op.join( PATH,npf))\n",
    "                    myFile = xr.open_dataset(op.join(ncPath,ncf))\n",
    "                    planck_fk1 = float(myFile['planck_fk1'].data)\n",
    "                    planck_fk2 = float(myFile['planck_fk2'].data) \n",
    "                    planck_bc1 = float(myFile['planck_bc1'].data)                       \n",
    "                    planck_bc2 = float(myFile['planck_bc2'].data)\n",
    "\n",
    "                    time = ncf[31:38]\n",
    "                    G17_mean = Rad2BT(imageBox.mean(), planck_fk1, planck_fk2, planck_bc1, planck_bc2)\n",
    "                    G17_std = Rad2BT(imageBox.std(), planck_fk1, planck_fk2, planck_bc1, planck_bc2)\n",
    "\n",
    "                    hh = ncf[34:36]\n",
    "                    mm = ncf[36:38]\n",
    "\n",
    "                    print(str(ncf), file=log)\n",
    "                    logger.info(str(ncf))\n",
    "\n",
    "                    G16_npy = np.load( op.join(PATH, npf.replace('G16','G17',1)) )\n",
    "                    G16_ncf = xr.open_dataset(op.join( ncPath, ncf.replace('G16','G17',1) ))\n",
    "                    G16_fk1 = float(G16_ncf['planck_fk1'].data)\n",
    "                    G16_fk2 = float(G16_ncf['planck_fk2'].data) \n",
    "                    G16_bc1 = float(G16_ncf['planck_bc1'].data)                       \n",
    "                    G16_bc2 = float(G16_ncf['planck_bc2'].data)\n",
    "\n",
    "                    target_G16_Temp = Rad2BT(G16_npy, G16_fk1, G16_fk2, G16_bc1, G16_bc2)\n",
    "                    x,y = imageBox.shape[0],imageBox.shape[1]\n",
    "\n",
    "                    #select \"10,000\" random points out of 401,401\n",
    "                    sample = np.array([combo for combo in itertools.product(range(x),range(y))])\n",
    "                    inds=nr.randint(x*y, size=(samp))\n",
    "                    inds.sort()\n",
    "                    \n",
    "                    for i,j in sample[inds]:#bottleneck for time\n",
    "                        lon = longitude( extent_pc[0], extent_pc[1], i, x )\n",
    "                        lat = latitude( extent_pc[2], extent_pc[3], j, y)\n",
    "\n",
    "                        G17_Temp = Rad2BT(imageBox[i,j], planck_fk1, planck_fk2, planck_bc1, planck_bc2)#nonfilter\n",
    "                        target_G16_Temp = Rad2BT( G16_npy[i,j], G16_fk1, G16_fk2, G16_bc1, G16_bc2 )\n",
    "\n",
    "                        row = [time, lon, lat, G17_Temp, G17_mean, G17_std, target_G16_Temp]\n",
    "\n",
    "                        dataFrames.append(pd.DataFrame([row],columns=columns))\n",
    "                        #concatenate a list of single dataframes after done with loop for speed \" pandas.concat\"\n",
    "                        #z score versus the standard deviation for single image, can normalize across all image\n",
    "                    \n",
    "                    df=pd.concat(dataFrames)\n",
    "                    df = df.dropna()\n",
    "                    df = df[ df[\"G17_Temp\"] > 0]\n",
    "                    df = df[ df[\"target_G16_Temp\"] > 0]\n",
    "\n",
    "                    from sklearn.neural_network import MLPRegressor\n",
    "                    #from sklearn.datasets import make_regression\n",
    "                    from sklearn.model_selection import train_test_split\n",
    "                    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "                    dfSample = df.sample(n=samp)\n",
    "                    X = dfSample.drop([\"target_G16_Temp\"],axis=1).astype('float')#try as float\n",
    "                    y = dfSample.drop(columns[:-1],axis=1).astype('float')\n",
    "\n",
    "                    from sklearn.metrics import mean_squared_error\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X, np.ravel(y),random_state=1,test_size=0.0)\n",
    "                    regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n",
    "                    #We have enough data so we don't necessarily need to leave a percentage aside for testing\n",
    "                    \n",
    "                    '''Scaled data frame retaining columns whose values vary'''\n",
    "                    X_test = pd.DataFrame([X_test],columns = columns[-1])\n",
    "                    \n",
    "                    \n",
    "#                     scalarTest = StandardScaler()\n",
    "#                     scalarTest.fit(X_test)\n",
    "#                     scalarTest.transform(X_test)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    print('Working on samples of size:', samp)\n",
    "                    if int(DD) == 206:#non pCal\n",
    "                        print('Working on MLP sample')\n",
    "                        \n",
    "                        X_test = dfFixed_ML.drop([\"target_G16_Temp\"],axis=1).astype('float')\n",
    "                        y_test = np.ravel(dfFixed_ML.drop(columns[:-1],axis=1).astype('float'))\n",
    "\n",
    "                        train = regr.score(X_train,y_train)\n",
    "                        test = regr.score(X_test, y_test)\n",
    "\n",
    "                        R2_ML.append(test)\n",
    "                        R2_ML_train_test.append(train/test)\n",
    "\n",
    "                        prediction = regr.predict(X_test)\n",
    "                        errors = abs(prediction - y_test)\n",
    "\n",
    "                        MAE = round(np.mean(errors), 2)\n",
    "                        MAE_ML.append(MAE)\n",
    "\n",
    "                        MSE = mean_squared_error(prediction, y_test)\n",
    "                        MSE_ML.append(MSE)\n",
    "\n",
    "                        print(MAE_ML,file=log) \n",
    "                        print(MSE_ML,file=log)\n",
    "                        print(R2_ML,file=log)\n",
    "                        print(R2_ML_train_test,file=log)\n",
    "                        \n",
    "                        dfTarget_ML.append(y_test)\n",
    "                        dfPredict_ML.append(prediction)\n",
    "\n",
    "                    if int(DD) == 207:#pCal        \n",
    "                        print('Working on pCal sample')\n",
    "                        #dfTest_pCal = df.sample(samp)\n",
    "                        X_test = dfFixed_pCal.drop([\"target_G16_Temp\"],axis=1).astype('float')\n",
    "                        y_test = np.ravel(dfFixed_pCal.drop(columns[:-1],axis=1).astype('float'))\n",
    "\n",
    "                        train = regr.score(X_train,y_train)\n",
    "                        test = regr.score(X_test, y_test)\n",
    "\n",
    "                        R2_pCal.append(test)\n",
    "                        R2_pCal_train_test.append(train/test)\n",
    "\n",
    "                        prediction = regr.predict(X_test)\n",
    "                        errors = abs(prediction - y_test)\n",
    "\n",
    "                        MAE = round(np.mean(errors), 2)\n",
    "                        MAE_pCal.append(MAE)\n",
    "\n",
    "                        MSE = mean_squared_error(prediction, y_test)\n",
    "                        MSE_pCal.append(MSE)\n",
    "\n",
    "                        print(MAE_pCal,file=log) \n",
    "                        print(MSE_pCal,file=log)\n",
    "                        print(R2_pCal,file=log)\n",
    "                        print(R2_pCal_train_test,file=log)\n",
    "                        \n",
    "                        dfTarget_pCal.append(y_test)\n",
    "                        dfPredict_pCal.append(prediction)\n",
    "\n",
    "                \n",
    "                except ValueError as e:\n",
    "                    logger.exception(e)\n",
    "                    print(e)\n",
    "                    print(e, file=log)\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTarget_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pairs[0][0]))\n",
    "print(pairs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (ML, pCal) in enumerate(pairs):   \n",
    "#     for j, label in enumerate(name):\n",
    "#         plt.scatter(points, pairs[i][j], label=label)\n",
    "#     plt.title(titles[i])\n",
    "#     plt.legend(loc=\"center right\")\n",
    "#     plt.xlabel(\"sample size\")\n",
    "#     plt.ylabel(ylabels[i])\n",
    "    \n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "points =['1000','10000','100000']\n",
    "\n",
    "pairs_ML = (dfTarget_ML[1:],dfPredict_ML[1:])\n",
    "pairs_pCal = (dfTarget_pCal[1:], dfPredict_pCal[1:])\n",
    "pairs = (pairs_ML,pairs_pCal)\n",
    "\n",
    "\n",
    "name = ['MLP','pCal']\n",
    "#for i, (ML, pCal) in enumerate(pairs):\n",
    "for color in points:\n",
    "    i = points.index(color)\n",
    "    for j, label in enumerate(name):\n",
    "        plt.scatter(pairs[j][0][i], pairs[j][1][i], alpha=0.1, label=label)\n",
    "    plt.title(f'Scatter plot on fit for {color} sample points')\n",
    "    plt.xlabel(\"Target Values\")\n",
    "    plt.ylabel(\"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MAE_ML) \n",
    "print(MSE_ML)\n",
    "print(R2_ML)\n",
    "print(R2_ML_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MAE_pCal) \n",
    "print(MSE_pCal)\n",
    "print(R2_pCal)\n",
    "print(R2_pCal_train_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points =['100','1000','10000','100000']\n",
    "points =['1000','10000','100000']#Interpretable scale for minimum training points\n",
    "# metric_pCal = [MAE_pCal, MSE_pCal]\n",
    "# metric_ML = [MAE_ML, MSE_ML]\n",
    "metric_pCal = [MAE_pCal[1:], MSE_pCal[1:]]\n",
    "metric_ML = [MAE_ML[1:], MSE_ML[1:]]\n",
    "# metric_pCal = [MAE_pCal, MSE_pCal, R2_pCal, R2_pCal_train_test]\n",
    "# metric_ML = [MAE_ML, MSE_ML, R2_ML, R2_ML_train_test]\n",
    "metric = [metric_ML, metric_pCal]\n",
    "name = ['MLP','pCal']\n",
    "ylabels = ['Kelvin','Kelvin^2','R-square','R^2_train/ R^2_test']\n",
    "titles = ['Mean Absolute Error MLP vs pCal',\n",
    "          'Mean Square Error MLP vs pCal',\n",
    "          'R-Square MLP vs pCal',\n",
    "          'R-Square ratios of MLP vs pCal']\n",
    "\n",
    "pairs = list(zip(metric_ML,metric_pCal))\n",
    "for i, (ML, pCal) in enumerate(pairs):   \n",
    "    for j, label in enumerate(name):\n",
    "        plt.plot(points, pairs[i][j], label=label)\n",
    "    plt.title(titles[i])\n",
    "    plt.legend(loc=\"center right\")\n",
    "    plt.xlabel(\"sample size\")\n",
    "    plt.ylabel(ylabels[i])\n",
    "    \n",
    "    \n",
    "    plt.show()   \n",
    "#for i in range(len(points)):    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric = ['MAE','MSE','R-Square','Validation ratio']\n",
    "metric = ['MAE','MSE']\n",
    "for i,j in itertools.product(range(len(metric)), range(len(metric))):\n",
    "    metric_pCal[i][j] = round(metric_pCal[i][j],2)\n",
    "    metric_ML[i][j] = round(metric_ML[i][j],2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric_ML)\n",
    "print(metric_pCal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "for i in range(len(trainSamp)):\n",
    "    fig = go.Figure(data=[go.Table(header=dict(values=[f'Metric {points[i]}pts','MLP', 'pCal']),\n",
    "                     cells=dict(values=[metric, np.array(metric_ML).T[i],  np.array(metric_pCal).T[i]]))\n",
    "                         ])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
